{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-05feba2eb580>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-05feba2eb580>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [docs]class GAN(TensorGraph):\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[docs]class GAN(TensorGraph):\n",
    "  \"\"\"Implements Generative Adversarial Networks.\n",
    "\n",
    "  A Generative Adversarial Network (GAN) is a type of generative model.  It\n",
    "  consists of two parts called the \"generator\" and the \"discriminator\".  The\n",
    "  generator takes random noise as input and transforms it into an output that\n",
    "  (hopefully) resembles the training data.  The discriminator takes a set of\n",
    "  samples as input and tries to distinguish the real training samples from the\n",
    "  ones created by the generator.  Both of them are trained together.  The\n",
    "  discriminator tries to get better and better at telling real from false data,\n",
    "  while the generator tries to get better and better at fooling the discriminator.\n",
    "\n",
    "  In many cases there also are additional inputs to the generator and\n",
    "  discriminator.  In that case it is known as a Conditional GAN (CGAN), since it\n",
    "  learns a distribution that is conditional on the values of those inputs.  They\n",
    "  are referred to as \"conditional inputs\".\n",
    "\n",
    "  Many variations on this idea have been proposed, and new varieties of GANs are\n",
    "  constantly being proposed.  This class tries to make it very easy to implement\n",
    "  straightforward GANs of the most conventional types.  At the same time, it\n",
    "  tries to be flexible enough that it can be used to implement many (but\n",
    "  certainly not all) variations on the concept.\n",
    "\n",
    "  To define a GAN, you must create a subclass that provides implementations of\n",
    "  the following methods:\n",
    "\n",
    "  get_noise_input_shape()\n",
    "  get_data_input_shapes()\n",
    "  create_generator()\n",
    "  create_discriminator()\n",
    "\n",
    "  If you want your GAN to have any conditional inputs you must also implement:\n",
    "\n",
    "  get_conditional_input_shapes()\n",
    "\n",
    "  The following methods have default implementations that are suitable for most\n",
    "  conventional GANs.  You can override them if you want to customize their\n",
    "  behavior:\n",
    "\n",
    "  create_generator_loss()\n",
    "  create_discriminator_loss()\n",
    "  get_noise_batch()\n",
    "\n",
    "  This class allows a GAN to have multiple generators and discriminators, a model\n",
    "  known as MIX+GAN.  It is described in Arora et al., \"Generalization and\n",
    "  Equilibrium in Generative Adversarial Nets (GANs)\" (https://arxiv.org/abs/1703.00573).\n",
    "  This can lead to better models, and is especially useful for reducing mode\n",
    "  collapse, since different generators can learn different parts of the\n",
    "  distribution.  To use this technique, simply specify the number of generators\n",
    "  and discriminators when calling the constructor.  You can then tell\n",
    "  predict_gan_generator() which generator to use for predicting samples.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_generators=1, n_discriminators=1, **kwargs):\n",
    "    \"\"\"Construct a GAN.\n",
    "\n",
    "    In addition to the parameters listed below, this class accepts all the\n",
    "    keyword arguments from TensorGraph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_generators: int\n",
    "      the number of generators to include\n",
    "    n_discriminators: int\n",
    "      the number of discriminators to include\n",
    "    \"\"\"\n",
    "    super(GAN, self).__init__(use_queue=False, **kwargs)\n",
    "    self.n_generators = n_generators\n",
    "    self.n_discriminators = n_discriminators\n",
    "\n",
    "    # Create the inputs.\n",
    "\n",
    "    self.noise_input = layers.Feature(shape=self.get_noise_input_shape())\n",
    "    self.data_inputs = []\n",
    "    for shape in self.get_data_input_shapes():\n",
    "      self.data_inputs.append(layers.Feature(shape=shape))\n",
    "    self.conditional_inputs = []\n",
    "    for shape in self.get_conditional_input_shapes():\n",
    "      self.conditional_inputs.append(layers.Feature(shape=shape))\n",
    "\n",
    "    # Create the generators.\n",
    "\n",
    "    self.generators = []\n",
    "    for i in range(n_generators):\n",
    "      generator = self.create_generator(self.noise_input,\n",
    "                                        self.conditional_inputs)\n",
    "      if not isinstance(generator, Sequence):\n",
    "        raise ValueError('create_generator() must return a list of Layers')\n",
    "      if len(generator) != len(self.data_inputs):\n",
    "        raise ValueError(\n",
    "            'The number of generator outputs must match the number of data inputs'\n",
    "        )\n",
    "      for g, d in zip(generator, self.data_inputs):\n",
    "        if g.shape != d.shape:\n",
    "          raise ValueError(\n",
    "              'The shapes of the generator outputs must match the shapes of the data inputs'\n",
    "          )\n",
    "      for g in generator:\n",
    "        self.add_output(g)\n",
    "      self.generators.append(generator)\n",
    "\n",
    "    # Create the discriminators.\n",
    "\n",
    "    self.discrim_train = []\n",
    "    self.discrim_gen = []\n",
    "    for i in range(n_discriminators):\n",
    "      discrim_train = self.create_discriminator(self.data_inputs,\n",
    "                                                self.conditional_inputs)\n",
    "      self.discrim_train.append(discrim_train)\n",
    "\n",
    "      # Make a copy of the discriminator that takes each generator's output as\n",
    "      # its input.\n",
    "\n",
    "      for generator in self.generators:\n",
    "        replacements = {}\n",
    "        for g, d in zip(generator, self.data_inputs):\n",
    "          replacements[d] = g\n",
    "        for c in self.conditional_inputs:\n",
    "          replacements[c] = c\n",
    "        discrim_gen = discrim_train.copy(replacements, shared=True)\n",
    "        self.discrim_gen.append(discrim_gen)\n",
    "\n",
    "    # Make a list of all layers in the generators and discriminators.\n",
    "\n",
    "    def add_layers_to_set(layer, layers):\n",
    "      if layer not in layers:\n",
    "        layers.add(layer)\n",
    "        for i in layer.in_layers:\n",
    "          add_layers_to_set(i, layers)\n",
    "\n",
    "    gen_layers = set()\n",
    "    for generator in self.generators:\n",
    "      for layer in generator:\n",
    "        add_layers_to_set(layer, gen_layers)\n",
    "    discrim_layers = set()\n",
    "    for discriminator in self.discrim_train:\n",
    "      add_layers_to_set(discriminator, discrim_layers)\n",
    "    discrim_layers -= gen_layers\n",
    "\n",
    "    # Compute the loss functions.\n",
    "\n",
    "    gen_losses = [self.create_generator_loss(d) for d in self.discrim_gen]\n",
    "    discrim_losses = []\n",
    "    for i in range(n_discriminators):\n",
    "      for j in range(n_generators):\n",
    "        discrim_losses.append(\n",
    "            self.create_discriminator_loss(\n",
    "                self.discrim_train[i], self.discrim_gen[i * n_generators + j]))\n",
    "    if n_generators == 1 and n_discriminators == 1:\n",
    "      total_gen_loss = gen_losses[0]\n",
    "      total_discrim_loss = discrim_losses[0]\n",
    "    else:\n",
    "      # Create learnable weights for the generators and discriminators.\n",
    "\n",
    "      gen_alpha = layers.Variable(np.ones((1, n_generators)))\n",
    "      gen_weights = layers.SoftMax(gen_alpha)\n",
    "      discrim_alpha = layers.Variable(np.ones((1, n_discriminators)))\n",
    "      discrim_weights = layers.SoftMax(discrim_alpha)\n",
    "\n",
    "      # Compute the weighted errors\n",
    "\n",
    "      weight_products = layers.Reshape(\n",
    "          (n_generators * n_discriminators,),\n",
    "          in_layers=layers.Reshape(\n",
    "              (n_discriminators,\n",
    "               1), in_layers=discrim_weights) * layers.Reshape(\n",
    "                   (1, n_generators), in_layers=gen_weights))\n",
    "      total_gen_loss = layers.WeightedError((layers.Stack(gen_losses, axis=0),\n",
    "                                             weight_products))\n",
    "      total_discrim_loss = layers.WeightedError((layers.Stack(\n",
    "          discrim_losses, axis=0), weight_products))\n",
    "      gen_layers.add(gen_alpha)\n",
    "      discrim_layers.add(gen_alpha)\n",
    "      discrim_layers.add(discrim_alpha)\n",
    "\n",
    "      # Add an entropy term to the loss.\n",
    "\n",
    "      entropy = -(\n",
    "          layers.ReduceSum(layers.Log(gen_weights)) / n_generators +\n",
    "          layers.ReduceSum(layers.Log(discrim_weights)) / n_discriminators)\n",
    "      total_discrim_loss += entropy\n",
    "\n",
    "    # Create submodels for training the generators and discriminators.\n",
    "\n",
    "    self.generator_submodel = self.create_submodel(\n",
    "        layers=gen_layers, loss=total_gen_loss)\n",
    "    self.discriminator_submodel = self.create_submodel(\n",
    "        layers=discrim_layers, loss=total_discrim_loss)\n",
    "\n",
    "[docs]  def get_noise_input_shape(self):\n",
    "    \"\"\"Get the shape of the generator's noise input layer.\n",
    "\n",
    "    Subclasses must override this to return a tuple giving the shape of the\n",
    "    noise input.  The actual Input layer will be created automatically.  The\n",
    "    first dimension must be None, since it will correspond to the batch size.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Subclasses must implement this.\")\n",
    "\n",
    "\n",
    "[docs]  def get_data_input_shapes(self):\n",
    "    \"\"\"Get the shapes of the inputs for training data.\n",
    "\n",
    "    Subclasses must override this to return a list of tuples, each giving the\n",
    "    shape of one of the inputs.  The actual Input layers will be created\n",
    "    automatically.  This list of shapes must also match the shapes of the\n",
    "    generator's outputs.  The first dimension of each shape must be None, since\n",
    "    it will correspond to the batch size.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Subclasses must implement this.\")\n",
    "\n",
    "\n",
    "[docs]  def get_conditional_input_shapes(self):\n",
    "    \"\"\"Get the shapes of any conditional inputs.\n",
    "\n",
    "    Subclasses may override this to return a list of tuples, each giving the\n",
    "    shape of one of the conditional inputs.  The actual Input layers will be\n",
    "    created automatically.  The first dimension of each shape must be None,\n",
    "    since it will correspond to the batch size.\n",
    "\n",
    "    The default implementation returns an empty list, meaning there are no\n",
    "    conditional inputs.\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "\n",
    "[docs]  def get_noise_batch(self, batch_size):\n",
    "    \"\"\"Get a batch of random noise to pass to the generator.\n",
    "\n",
    "    This should return a NumPy array whose shape matches the one returned by\n",
    "    get_noise_input_shape().  The default implementation returns normally\n",
    "    distributed values.  Subclasses can override this to implement a different\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    size = list(self.get_noise_input_shape())\n",
    "    size[0] = batch_size\n",
    "    return np.random.normal(size=size)\n",
    "\n",
    "\n",
    "[docs]  def create_generator(self, noise_input, conditional_inputs):\n",
    "    \"\"\"Create the generator.\n",
    "\n",
    "    Subclasses must override this to construct the generator and return its\n",
    "    output layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    noise_input: Input\n",
    "      the Input layer from which the generator can read random noise.  The shape\n",
    "      will match the return value from get_noise_input_shape().\n",
    "    conditional_inputs: list\n",
    "      the Input layers for any conditional inputs to the network.  The number\n",
    "      and shapes of these inputs will match the return value from\n",
    "      get_conditional_input_shapes().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of Layer objects that produce the generator's outputs.  The number and\n",
    "    shapes of these layers must match the return value from get_data_input_shapes(),\n",
    "    since generated data must have the same form as training data.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Subclasses must implement this.\")\n",
    "\n",
    "\n",
    "[docs]  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    \"\"\"Create the discriminator.\n",
    "\n",
    "    Subclasses must override this to construct the discriminator and return its\n",
    "    output layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_inputs: list\n",
    "      the Input layers from which the discriminator can read the input data.\n",
    "      The number and shapes of these inputs will match the return value from\n",
    "      get_data_input_shapes().  The samples read from these layers may be either\n",
    "      training data or generated data.\n",
    "    conditional_inputs: list\n",
    "      the Input layers for any conditional inputs to the network.  The number\n",
    "      and shapes of these inputs will match the return value from\n",
    "      get_conditional_input_shapes().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Layer object that outputs the probability of each sample being a training\n",
    "    sample.  The shape of this layer must be [None].  That is, it must output a\n",
    "    one dimensional tensor whose length equals the batch size.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Subclasses must implement this.\")\n",
    "\n",
    "\n",
    "[docs]  def create_generator_loss(self, discrim_output):\n",
    "    \"\"\"Create the loss function for the generator.\n",
    "\n",
    "    The default implementation is appropriate for most cases.  Subclasses can\n",
    "    override this if the need to customize it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    discrim_output: Layer\n",
    "      the output from the discriminator on a batch of generated data.  This is\n",
    "      its estimate of the probability that each sample is training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Layer object that outputs the loss function to use for optimizing the\n",
    "    generator.\n",
    "    \"\"\"\n",
    "    return -layers.ReduceMean(layers.Log(discrim_output + 1e-10))\n",
    "\n",
    "\n",
    "[docs]  def create_discriminator_loss(self, discrim_output_train, discrim_output_gen):\n",
    "    \"\"\"Create the loss function for the discriminator.\n",
    "\n",
    "    The default implementation is appropriate for most cases.  Subclasses can\n",
    "    override this if the need to customize it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    discrim_output_train: Layer\n",
    "      the output from the discriminator on a batch of generated data.  This is\n",
    "      its estimate of the probability that each sample is training data.\n",
    "    discrim_output_gen: Layer\n",
    "      the output from the discriminator on a batch of training data.  This is\n",
    "      its estimate of the probability that each sample is training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Layer object that outputs the loss function to use for optimizing the\n",
    "    discriminator.\n",
    "    \"\"\"\n",
    "    training_data_loss = layers.Log(discrim_output_train + 1e-10)\n",
    "    gen_data_loss = layers.Log(1 - discrim_output_gen + 1e-10)\n",
    "    return -layers.ReduceMean(training_data_loss + gen_data_loss)\n",
    "\n",
    "\n",
    "[docs]  def fit_gan(self,\n",
    "              batches,\n",
    "              generator_steps=1.0,\n",
    "              max_checkpoints_to_keep=5,\n",
    "              checkpoint_interval=1000,\n",
    "              restore=False):\n",
    "    \"\"\"Train this model on data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batches: iterable\n",
    "      batches of data to train the discriminator on, each represented as a dict\n",
    "      that maps Layers to values.  It should specify values for all members of\n",
    "      data_inputs and conditional_inputs.\n",
    "    generator_steps: float\n",
    "      the number of training steps to perform for the generator for each batch.\n",
    "      This can be used to adjust the ratio of training steps for the generator\n",
    "      and discriminator.  For example, 2.0 will perform two training steps for\n",
    "      every batch, while 0.5 will only perform one training step for every two\n",
    "      batches.\n",
    "    max_checkpoints_to_keep: int\n",
    "      the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
    "    checkpoint_interval: int\n",
    "      the frequency at which to write checkpoints, measured in batches.  Set\n",
    "      this to 0 to disable automatic checkpointing.\n",
    "    restore: bool\n",
    "      if True, restore the model from the most recent checkpoint before training\n",
    "      it.\n",
    "    \"\"\"\n",
    "    if not self.built:\n",
    "      self.build()\n",
    "    if restore:\n",
    "      self.restore()\n",
    "    gen_train_fraction = 0.0\n",
    "    discrim_error = 0.0\n",
    "    gen_error = 0.0\n",
    "    discrim_average_steps = 0\n",
    "    gen_average_steps = 0\n",
    "    time1 = time.time()\n",
    "    with self._get_tf(\"Graph\").as_default():\n",
    "      if checkpoint_interval > 0:\n",
    "        saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep)\n",
    "      for feed_dict in batches:\n",
    "        # Every call to fit_generator() will increment global_step, but we only\n",
    "        # want it to get incremented once for the entire batch, so record the\n",
    "        # value and keep resetting it.\n",
    "\n",
    "        global_step = self.global_step\n",
    "\n",
    "        # Train the discriminator.\n",
    "\n",
    "        feed_dict = dict(feed_dict)\n",
    "        feed_dict[self.noise_input] = self.get_noise_batch(self.batch_size)\n",
    "        discrim_error += self.fit_generator(\n",
    "            [feed_dict],\n",
    "            submodel=self.discriminator_submodel,\n",
    "            checkpoint_interval=0)\n",
    "        self.global_step = global_step\n",
    "        discrim_average_steps += 1\n",
    "\n",
    "        # Train the generator.\n",
    "\n",
    "        if generator_steps > 0.0:\n",
    "          gen_train_fraction += generator_steps\n",
    "          while gen_train_fraction >= 1.0:\n",
    "            feed_dict[self.noise_input] = self.get_noise_batch(self.batch_size)\n",
    "            gen_error += self.fit_generator(\n",
    "                [feed_dict],\n",
    "                submodel=self.generator_submodel,\n",
    "                checkpoint_interval=0)\n",
    "            self.global_step = global_step\n",
    "            gen_average_steps += 1\n",
    "            gen_train_fraction -= 1.0\n",
    "        self.global_step = global_step + 1\n",
    "\n",
    "        # Write checkpoints and report progress.\n",
    "\n",
    "        if discrim_average_steps == checkpoint_interval:\n",
    "          saver.save(self.session, self.save_file, global_step=self.global_step)\n",
    "          discrim_loss = discrim_error / max(1, discrim_average_steps)\n",
    "          gen_loss = gen_error / max(1, gen_average_steps)\n",
    "          print(\n",
    "              'Ending global_step %d: generator average loss %g, discriminator average loss %g'\n",
    "              % (self.global_step, gen_loss, discrim_loss))\n",
    "          discrim_error = 0.0\n",
    "          gen_error = 0.0\n",
    "          discrim_average_steps = 0\n",
    "          gen_average_steps = 0\n",
    "\n",
    "      # Write out final results.\n",
    "\n",
    "      if checkpoint_interval > 0:\n",
    "        if discrim_average_steps > 0 and gen_average_steps > 0:\n",
    "          discrim_loss = discrim_error / discrim_average_steps\n",
    "          gen_loss = gen_error / gen_average_steps\n",
    "          print(\n",
    "              'Ending global_step %d: generator average loss %g, discriminator average loss %g'\n",
    "              % (self.global_step, gen_loss, discrim_loss))\n",
    "        saver.save(self.session, self.save_file, global_step=self.global_step)\n",
    "        time2 = time.time()\n",
    "        print(\"TIMING: model fitting took %0.3f s\" % (time2 - time1))\n",
    "\n",
    "\n",
    "[docs]  def predict_gan_generator(self,\n",
    "                            batch_size=1,\n",
    "                            noise_input=None,\n",
    "                            conditional_inputs=[],\n",
    "                            generator_index=0):\n",
    "    \"\"\"Use the GAN to generate a batch of samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int\n",
    "      the number of samples to generate.  If either noise_input or\n",
    "      conditional_inputs is specified, this argument is ignored since the batch\n",
    "      size is then determined by the size of that argument.\n",
    "    noise_input: array\n",
    "      the value to use for the generator's noise input.  If None (the default),\n",
    "      get_noise_batch() is called to generate a random input, so each call will\n",
    "      produce a new set of samples.\n",
    "    conditional_inputs: list of arrays\n",
    "      the values to use for all conditional inputs.  This must be specified if\n",
    "      the GAN has any conditional inputs.\n",
    "    generator_index: int\n",
    "      the index of the generator (between 0 and n_generators-1) to use for\n",
    "      generating the samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An array (if the generator has only one output) or list of arrays (if it has\n",
    "    multiple outputs) containing the generated samples.\n",
    "    \"\"\"\n",
    "    if noise_input is not None:\n",
    "      batch_size = len(noise_input)\n",
    "    elif len(conditional_inputs) > 0:\n",
    "      batch_size = len(conditional_inputs[0])\n",
    "    if noise_input is None:\n",
    "      noise_input = self.get_noise_batch(batch_size)\n",
    "    batch = {}\n",
    "    batch[self.noise_input] = noise_input\n",
    "    for layer, value in zip(self.conditional_inputs, conditional_inputs):\n",
    "      batch[layer] = value\n",
    "    return self.predict_on_generator(\n",
    "        [batch], outputs=self.generators[generator_index])\n",
    "\n",
    "\n",
    "  def _set_empty_inputs(self, feed_dict, layers):\n",
    "    \"\"\"Set entries in a feed dict corresponding to a batch size of 0.\"\"\"\n",
    "    for layer in layers:\n",
    "      shape = list(layer.shape)\n",
    "      shape[0] = 0\n",
    "      feed_dict[layer] = np.zeros(shape)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class WGAN(GAN):\n",
    "  \"\"\"Implements Wasserstein Generative Adversarial Networks.\n",
    "\n",
    "  This class implements Wasserstein Generative Adversarial Networks (WGANs) as\n",
    "  described in Arjovsky et al., \"Wasserstein GAN\" (https://arxiv.org/abs/1701.07875).\n",
    "  A WGAN is conceptually rather different from a conventional GAN, but in\n",
    "  practical terms very similar.  It reinterprets the discriminator (often called\n",
    "  the \"critic\" in this context) as learning an approximation to the Earth Mover\n",
    "  distance between the training and generated distributions.  The generator is\n",
    "  then trained to minimize that distance.  In practice, this just means using\n",
    "  slightly different loss functions for training the generator and discriminator.\n",
    "\n",
    "  WGANs have theoretical advantages over conventional GANs, and they often work\n",
    "  better in practice.  In addition, the discriminator's loss function can be\n",
    "  directly interpreted as a measure of the quality of the model.  That is an\n",
    "  advantage over conventional GANs, where the loss does not directly convey\n",
    "  information about the quality of the model.\n",
    "\n",
    "  The theory WGANs are based on requires the discriminator's gradient to be\n",
    "  bounded.  The original paper achieved this by clipping its weights.  This\n",
    "  class instead does it by adding a penalty term to the discriminator's loss, as\n",
    "  described in https://arxiv.org/abs/1704.00028.  This is sometimes found to\n",
    "  produce better results.\n",
    "\n",
    "  There are a few other practical differences between GANs and WGANs.  In a\n",
    "  conventional GAN, the discriminator's output must be between 0 and 1 so it can\n",
    "  be interpreted as a probability.  In a WGAN, it should produce an unbounded\n",
    "  output that can be interpreted as a distance.\n",
    "\n",
    "  When training a WGAN, you also should usually use a smaller value for\n",
    "  generator_steps.  Conventional GANs rely on keeping the generator and\n",
    "  discriminator \"in balance\" with each other.  If the discriminator ever gets\n",
    "  too good, it becomes impossible for the generator to fool it and training\n",
    "  stalls.  WGANs do not have this problem, and in fact the better the\n",
    "  discriminator is, the easier it is for the generator to improve.  It therefore\n",
    "  usually works best to perform several training steps on the discriminator for\n",
    "  each training step on the generator.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gradient_penalty=10.0, **kwargs):\n",
    "    \"\"\"Construct a WGAN.\n",
    "\n",
    "    In addition to the following, this class accepts all the keyword arguments\n",
    "    from TensorGraph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gradient_penalty: float\n",
    "      the magnitude of the gradient penalty loss\n",
    "    \"\"\"\n",
    "    super(WGAN, self).__init__(**kwargs)\n",
    "    self.gradient_penalty = gradient_penalty\n",
    "\n",
    "[docs]  def create_generator_loss(self, discrim_output):\n",
    "    return layers.ReduceMean(discrim_output)\n",
    "\n",
    "\n",
    "[docs]  def create_discriminator_loss(self, discrim_output_train, discrim_output_gen):\n",
    "    gradient_penalty = GradientPenaltyLayer(discrim_output_train, self)\n",
    "    return gradient_penalty + layers.ReduceMean(discrim_output_train -\n",
    "                                                discrim_output_gen)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class GradientPenaltyLayer(layers.Layer):\n",
    "  \"\"\"Implements the gradient penalty loss term for WGANs.\"\"\"\n",
    "\n",
    "  def __init__(self, discrim_output_train, gan):\n",
    "    super(GradientPenaltyLayer, self).__init__([discrim_output_train])\n",
    "    self.gan = gan\n",
    "\n",
    "[docs]  def create_tensor(self, in_layers=None, set_tensors=True, **kwargs):\n",
    "    gradients = tf.gradients(self.in_layers[0], self.gan.data_inputs)\n",
    "    norm2 = 0.0\n",
    "    for g in gradients:\n",
    "      g2 = tf.square(g)\n",
    "      dims = len(g.shape)\n",
    "      if dims > 1:\n",
    "        g2 = tf.reduce_sum(g2, axis=list(range(1, dims)))\n",
    "      norm2 += g2\n",
    "    penalty = tf.square(tf.sqrt(norm2) - 1.0)\n",
    "    self.out_tensor = self.gan.gradient_penalty * tf.reduce_mean(penalty)\n",
    "    return self.out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldemarchi/anaconda3/envs/deepchem/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/danieldemarchi/anaconda3/envs/deepchem/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generative Adversarial Networks.\"\"\"\n",
    "\n",
    "from deepchem.models import TensorGraph\n",
    "from deepchem.models.tensorgraph import layers\n",
    "from collections import Sequence\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import deepchem as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/peastman/deepchem/blob/e026c2f495ebeea46acee7443d8305bddcf74b38/deepchem/models/tensorgraph/tests/test_gan.py\n",
    "\n",
    "#Reference this if you ever need help setting up your model and shit\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from deepchem.models.tensorgraph import layers\n",
    "\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "  #Random gaussian data generator\n",
    "    means = 10 * np.random.random([batch_size, 1])\n",
    "    values = np.random.normal(means, scale=2.0)\n",
    "    return means, values\n",
    "\n",
    "\n",
    "def generate_data(gan, batches, batch_size):\n",
    "    for i in range(batches):\n",
    "        means, values = generate_batch(batch_size)\n",
    "        batch = {gan.data_inputs[0]: values, gan.conditional_inputs[0]: means}\n",
    "        yield batch\n",
    "\n",
    "class ExampleGAN(dc.models.GAN):\n",
    "\n",
    "    def get_noise_input_shape(self):\n",
    "        return (None, 2)\n",
    "\n",
    "    def get_data_input_shapes(self):\n",
    "        return [(None, 1)]\n",
    "\n",
    "    def get_conditional_input_shapes(self):\n",
    "        return [(None, 1)]\n",
    "\n",
    "    def create_generator(self, noise_input, conditional_inputs):\n",
    "        gen_in = layers.Concat([noise_input] + conditional_inputs)\n",
    "        return [layers.Dense(1, in_layers=gen_in)]\n",
    "\n",
    "    def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "        discrim_in = layers.Concat(data_inputs + conditional_inputs)\n",
    "        dense = layers.Dense(10, in_layers=discrim_in, activation_fn=tf.nn.relu)\n",
    "        return layers.Dense(1, in_layers=dense, activation_fn=tf.sigmoid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1619469749632595\n",
      "2.0654800292332105\n"
     ]
    }
   ],
   "source": [
    "gan = ExampleGAN(learning_rate=0.03, gradient_penalty=0.1)\n",
    "gan.fit_gan(\n",
    "        generate_data(gan, 10000, 100),\n",
    "        generator_steps=0.1,\n",
    "        checkpoint_interval=0)\n",
    "\n",
    "# See if it has done a plausible job of learning the distribution.\n",
    "\n",
    "means = 10 * np.random.random([1000, 1])\n",
    "values = gan.predict_gan_generator(conditional_inputs=[means])\n",
    "deltas = values - means\n",
    "print(abs(np.mean(deltas)))\n",
    "print(np.std(deltas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 34.789 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 26.906 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 25.350 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 20.775 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 30.558 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.972 s\n",
      "TIMING: dataset construction took 141.035 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 5.051 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.415 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.777 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.595 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "hiv_tasks, hiv_datasets, transformers = dc.molnet.load_hiv(featurizer='ECFP', split='scaffold', reload=True)\n",
    "hiv_train_dataset, hiv_valid_dataset, hiv_test_dataset = hiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.9799028009077664]\n",
      "computed_metrics: [0.7465231848912404]\n",
      "computed_metrics: [0.7522417389289093]\n",
      "Train scores\n",
      "{'mean-roc_auc_score': 0.9799028009077664}\n",
      "Validation scores\n",
      "{'mean-roc_auc_score': 0.7465231848912404}\n",
      "Test scores\n",
      "{'mean-roc_auc_score': 0.7522417389289093}\n"
     ]
    }
   ],
   "source": [
    "#from deepchem.models.tensorgraph.progressive_multitask import ProgressiveMultitaskClassifier\n",
    "model = dc.models.tensorgraph.progressive_multitask.ProgressiveMultitaskClassifier(\n",
    "    len(hiv_tasks),\n",
    "    n_features,\n",
    "    layer_sizes=[1000],\n",
    "    dropouts=[.25],\n",
    "    learning_rate=0.0005,\n",
    "    batch_size=50)\n",
    "\n",
    "model.fit(hiv_train_dataset)\n",
    "model.save()\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(hiv_train_dataset, [metric], transformers)\n",
    "valid_scores = model.evaluate(hiv_valid_dataset, [metric], transformers)\n",
    "test_scores = model.evaluate(hiv_test_dataset, [metric], transformers)\n",
    "\n",
    "print(\"Train scores\")\n",
    "print(train_scores)\n",
    "\n",
    "print(\"Validation scores\")\n",
    "print(valid_scores)\n",
    "\n",
    "print(\"Test scores\")\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((32901, 1024), (32901, 1), (32901, 1), (32901,))\n"
     ]
    }
   ],
   "source": [
    "print(hiv_train_dataset.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_hiv(gan, batches, batch_size):\n",
    "    for b in hiv_train_dataset.iterbatches(400):\n",
    "        values = b.X\n",
    "        batch = {gan.data_inputs[0]: values}\n",
    "\n",
    "class HIVGAN(dc.models.GAN):\n",
    "\n",
    "    def get_noise_input_shape(self):\n",
    "        return (None, 1024)\n",
    "\n",
    "    def get_data_input_shapes(self):\n",
    "        return [(None, 1024)]\n",
    "\n",
    "    def create_generator(self, noise_input, conditional_inputs):\n",
    "        dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "        dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "        reshaped = layers.Reshape((None, 1024), in_layers=dense2)\n",
    "        return [reshaped]\n",
    "\n",
    "    def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "        conv = layers.Conv2D(num_outputs=1024, kernel_size=5, stride=2, activation_fn=dc.models.tensorgraph.model_ops.lrelu(0.2), normalizer_fn=tf.layers.batch_normalization, in_layers=data_inputs)\n",
    "        dense = layers.Dense(1024, in_layers=layers.Flatten(conv), activation_fn=tf.sigmoid)\n",
    "        return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_2 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 1024]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2fe51bb3b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhivgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHIVGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhivgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_data_hiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhiv_test_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhivgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_gan_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, submodel, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         self.default_generator(\n\u001b[1;32m    152\u001b[0m             dataset, epochs=nb_epoch, deterministic=deterministic),\n\u001b[0;32m--> 153\u001b[0;31m         max_checkpoints_to_keep, checkpoint_interval, restore, submodel)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   def fit_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, feed_dict_generator, max_checkpoints_to_keep, checkpoint_interval, restore, submodel)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_initial_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_initial_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_final_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_final_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/layers.py\u001b[0m in \u001b[0;36mcreate_tensor\u001b[0;34m(self, in_layers, set_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m           \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m         \u001b[0mout_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m           \u001b[0mout_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m           \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m                            \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                            str(x.get_shape().as_list()))\n\u001b[0m\u001b[1;32m   1135\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d_2 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 1024]"
     ]
    }
   ],
   "source": [
    "hivgan = HIVGAN(learning_rate=0.03, gradient_penalty=0.1)\n",
    "hivgan.fit(generate_data_hiv)\n",
    "\n",
    "testvals = hiv_test_dataset\n",
    "predvals = hivgan.predict_gan_generator()\n",
    "deltas = values - means\n",
    "print(abs(np.mean(deltas)))\n",
    "print(np.std(deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
