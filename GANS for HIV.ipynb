{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldemarchi/anaconda3/envs/deepchem/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/danieldemarchi/anaconda3/envs/deepchem/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generative Adversarial Networks.\"\"\"\n",
    "\n",
    "from deepchem.models import TensorGraph\n",
    "from deepchem.models.tensorgraph import layers\n",
    "from collections import Sequence\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import deepchem as dc\n",
    "from deepchem.data.datasets import NumpyDataset # import NumpyDataset\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "from deepchem.metrics import to_one_hot\n",
    "\n",
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph, TFWrapper\n",
    "from deepchem.models.tensorgraph.layers import Feature, Label, Weights, \\\n",
    "    WeightedError, Dense, Dropout, WeightDecay, Reshape, SoftMax, SoftMaxCrossEntropy, \\\n",
    "    L2Loss, ReduceSum, Concat, Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 28.452 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 35.083 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 26.859 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 25.283 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 20.778 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.510 s\n",
      "TIMING: dataset construction took 138.529 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.876 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.719 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.719 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.690 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "hiv_tasks, hiv_datasets, hiv_transformers = dc.molnet.load_hiv(featurizer='ECFP', split='random', reload=True)\n",
    "hiv_train_dataset, hiv_valid_dataset, hiv_test_dataset = hiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 1024)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 1024)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 1024), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(1024, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "gan = HIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Ending global_step 5000: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Ending global_step 10000: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "100\n",
      "Ending global_step 10890: generator average loss nan, discriminator average loss nan\n",
      "TIMING: model fitting took 5644.942 s\n"
     ]
    }
   ],
   "source": [
    "def iterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(gan.batch_size)\n",
    "    for batch in hiv_train_dataset.iterbatches(batch_size=gan.batch_size):\n",
    "      yield {gan.data_inputs[0]: batch[0]}\n",
    "\n",
    "gan.fit_gan(iterbatches(33), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gan.predict_gan_generator(batch_size=100000)\n",
    "rsamples = np.rint(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.878630254600454"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multitask_model = dc.models.MultitaskClassifier(\n",
    "    1,\n",
    "    n_features,\n",
    "    layer_sizes=[1000],\n",
    "    dropouts=[.25],\n",
    "    learning_rate=0.001,\n",
    "    batch_size=50)\n",
    "multitask_model.fit(hiv_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.rint(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "resa = multitask_model.predict_on_batch(samples)[:,0]\n",
    "print(np.count_nonzero(resa[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31119\n",
      "1782\n"
     ]
    }
   ],
   "source": [
    "blah = np.rint(multitask_model.predict_on_batch(hiv_train_dataset.X)[:,0])\n",
    "print(np.count_nonzero(blah[:,0]))\n",
    "print(np.count_nonzero(blah[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gan.predict_gan_generator(batch_size=32901)\n",
    "rsamples = np.rint(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33690624\n",
      "1282501\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(rsamples))\n",
    "print(np.count_nonzero(hiv_train_dataset.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 1024)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 1024)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 1024), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    prev_layer = layers.Dense(1024, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    for i in range(1, 5):\n",
    "        layer = Dense(\n",
    "          in_layers=prev_layer,\n",
    "          out_channels=1024,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          weights_initializer=TFWrapper(\n",
    "              tf.truncated_normal_initializer, stddev=0.02),\n",
    "          biases_initializer=TFWrapper(\n",
    "              tf.constant_initializer, value=1))\n",
    "        prev_layer = layer\n",
    "        top_multitask_layer = prev_layer\n",
    "        \n",
    "    final = layers.Dense(1, in_layers=top_multitask_layer, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "\n",
    "    return final\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "gan = HIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Ending global_step 200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 1000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 1200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 1400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 1600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 1800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 2000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 2200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 2400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 2600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 2800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 3000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 3200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 3400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 3600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 3800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 4000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 4200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 4400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 4600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 4800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 5000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 5200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 5400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 5600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 5800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 6000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 6200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 6400: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 6600: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 6800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 7000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 7200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 7400: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 7600: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 7800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 8000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 8200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 8400: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 8600: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 8800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 9000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 9200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 9400: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 9600: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 9800: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 10000: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 10200: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 10400: generator average loss nan, discriminator average loss nan\n",
      "100\n",
      "Ending global_step 10600: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 10800: generator average loss nan, discriminator average loss nan\n",
      "Ending global_step 10890: generator average loss nan, discriminator average loss nan\n",
      "TIMING: model fitting took 5851.428 s\n"
     ]
    }
   ],
   "source": [
    "def iterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(gan.batch_size)\n",
    "    for batch in hiv_train_dataset.iterbatches(batch_size=gan.batch_size):\n",
    "      yield {gan.data_inputs[0]: batch[0]}\n",
    "\n",
    "gan.fit_gan(iterbatches(33), generator_steps=1.5, checkpoint_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
