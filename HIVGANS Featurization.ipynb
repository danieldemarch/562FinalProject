{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generative Adversarial Networks.\"\"\"\n",
    "\n",
    "from deepchem.models import TensorGraph\n",
    "from deepchem.models.tensorgraph import layers\n",
    "from collections import Sequence\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import deepchem as dc\n",
    "from deepchem.data.datasets import NumpyDataset # import NumpyDataset\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "from deepchem.metrics import to_one_hot\n",
    "\n",
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph, TFWrapper\n",
    "from deepchem.models.tensorgraph.layers import Feature, Label, Weights, \\\n",
    "    WeightedError, Dense, Dropout, WeightDecay, Reshape, SoftMax, SoftMaxCrossEntropy, \\\n",
    "    L2Loss, ReduceSum, Concat, Stack\n",
    "import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from deepchem.feat import Featurizer\n",
    "\n",
    "\n",
    "class dRawFeaturizer(Featurizer):\n",
    "\n",
    "  def __init__(self, smiles=False):\n",
    "    self.smiles = smiles\n",
    "\n",
    "  def _featurize(self, mol):\n",
    "    from rdkit import Chem\n",
    "    if self.smiles:\n",
    "      return smiles\n",
    "    else:\n",
    "      return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hiv dataset loader.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import deepchem\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def dload_hiv(featurizer='ECFP', split='index', reload=True):\n",
    "  \"\"\"Load hiv datasets. Does not do train/test split\"\"\"\n",
    "  # Featurize hiv dataset\n",
    "  logger.info(\"About to featurize hiv dataset.\")\n",
    "  data_dir = deepchem.utils.get_data_dir()\n",
    "  if reload:\n",
    "    save_dir = os.path.join(data_dir, \"hiv/\" + featurizer + \"/\" + str(split))\n",
    "\n",
    "  dataset_file = os.path.join(data_dir, \"HIV.csv\")\n",
    "  if not os.path.exists(dataset_file):\n",
    "    deepchem.utils.download_url(\n",
    "        'http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/HIV.csv'\n",
    "    )\n",
    "\n",
    "  hiv_tasks = [\"HIV_active\"]\n",
    "\n",
    "  if reload:\n",
    "    loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(\n",
    "        save_dir)\n",
    "    if loaded:\n",
    "      return hiv_tasks, all_dataset, transformers\n",
    "\n",
    "  if featurizer == 'ECFPsmall':\n",
    "    featurizer = deepchem.feat.CircularFingerprint(size=1024)\n",
    "  elif featurizer == 'ECFPmedium':\n",
    "    featurizer = deepchem.feat.CircularFingerprint(size=4096)\n",
    "  elif featurizer == 'ECFPlarge':\n",
    "    featurizer = deepchem.feat.CircularFingerprint(size=16384)\n",
    "  elif featurizer == 'ECFP4':\n",
    "    featurizer = deepchem.feat.CircularFingerprint(radius=4, size = 1024)\n",
    "  elif featurizer == 'ECFP6':\n",
    "    featurizer = deepchem.feat.CircularFingerprint(radius=6, size = 1024)\n",
    "  elif featurizer == 'Raw':\n",
    "    featurizer = dRawFeaturizer()\n",
    "\n",
    "  loader = deepchem.data.CSVLoader(\n",
    "      tasks=hiv_tasks, smiles_field=\"smiles\", featurizer=featurizer)\n",
    "  dataset = loader.featurize(dataset_file, shard_size=8192)\n",
    "  # Initialize transformers\n",
    "  transformers = [\n",
    "      deepchem.trans.BalancingTransformer(transform_w=True, dataset=dataset)\n",
    "  ]\n",
    "\n",
    "  logger.info(\"About to transform data\")\n",
    "  for transformer in transformers:\n",
    "    dataset = transformer.transform(dataset)\n",
    "\n",
    "  if split == None:\n",
    "    return hiv_tasks, (dataset, None, None), transformers\n",
    "\n",
    "  splitters = {\n",
    "      'index': deepchem.splits.IndexSplitter(),\n",
    "      'random': deepchem.splits.RandomSplitter(),\n",
    "      'scaffold': deepchem.splits.ScaffoldSplitter(),\n",
    "      'butina': deepchem.splits.ButinaSplitter()\n",
    "  }\n",
    "  splitter = splitters[split]\n",
    "  train, valid, test = splitter.train_valid_test_split(dataset)\n",
    "\n",
    "  if reload:\n",
    "    deepchem.utils.save.save_dataset_to_disk(save_dir, train, valid, test,\n",
    "                                             transformers)\n",
    "  return hiv_tasks, (train, valid, test), transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELS TO USE\n",
    "#LOGREG, RANDOM FOREST, INFLUENCE RELEVANCE VECTOR, MULTITASK NETWORK \n",
    "\n",
    "#FEATURIZATIONS\n",
    "#RAW, ECFP, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.438 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 14.378 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 15.088 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 14.223 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 15.187 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.349 s\n",
      "TIMING: dataset construction took 74.766 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.099 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.271 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.351 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.126 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "eshiv_tasks, eshiv_datasets, eshiv_transformers = dload_hiv(featurizer='ECFPsmall', split='random', reload=True)\n",
    "eshiv_train_dataset, eshiv_valid_dataset, eshiv_test_dataset = eshiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 35.405 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 35.204 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 38.385 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 35.902 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 35.976 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.762 s\n",
      "TIMING: dataset construction took 185.371 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.174 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.654 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 5.390 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 4.978 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "emhiv_tasks, emhiv_datasets, emhiv_transformers = dload_hiv(featurizer='ECFPmedium', split='random', reload=True)\n",
    "emhiv_train_dataset, emhiv_valid_dataset, emhiv_test_dataset = emhiv_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 119.645 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 113.834 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 110.962 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 118.477 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 124.218 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 2.662 s\n",
      "TIMING: dataset construction took 603.115 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 35.364 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 35.424 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 20.732 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 20.077 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "elhiv_tasks, elhiv_datasets, elhiv_transformers = dload_hiv(featurizer='ECFPlarge', split='random', reload=True)\n",
    "elhiv_train_dataset, elhiv_valid_dataset, elhiv_test_dataset = elhiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 13.376 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 13.528 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 13.788 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 14.023 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 14.753 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.350 s\n",
      "TIMING: dataset construction took 70.967 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.069 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.965 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.121 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.098 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "e4hiv_tasks, e4hiv_datasets, e4hiv_transformers = dload_hiv(featurizer='ECFP4', split='random', reload=True)\n",
    "e4hiv_train_dataset, e4hiv_valid_dataset, e4hiv_test_dataset = e4hiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /var/folders/ph/d61mj9js3hvf9z7n92kk8y6c0000gn/T/HIV.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.214 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 14.752 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 14.732 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 14.867 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 14.563 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.356 s\n",
      "TIMING: dataset construction took 74.625 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.969 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.027 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.176 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.147 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "e6hiv_tasks, e6hiv_datasets, e6hiv_transformers = dload_hiv(featurizer='ECFP6', split='random', reload=True)\n",
    "e6hiv_train_dataset, e6hiv_valid_dataset, e6hiv_test_dataset = e6hiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "n_features = 1024\n",
    "rahiv_tasks, rahiv_datasets, rahiv_transformers = dload_hiv(featurizer='Raw', split='random', reload=True)\n",
    "rahiv_train_dataset, rahiv_valid_dataset, rahiv_test_dataset = rahiv_datasets\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN TWO GANS, ONE ON RAW, ONE ON ECFP. MAYBE EVEN TRY MULTIPLE ECFPS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMALLHIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 1024)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 1024)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 1024), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(1024, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "esmallgan = ESMALLHIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMEDIUMHIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 4096)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 4096)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(4096, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(4096, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(4096, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(4096, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(4096, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 4096), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(4096, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(4096, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(4096, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(4096, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(4096, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(4096, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "emediumgan = EMEDIUMHIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELARGEHIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 16384)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 16384)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(16384, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(16384, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(16384, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(16384, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(16384, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 16384), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(16384, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(16384, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(16384, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(16384, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(16384, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(16384, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "elargegan = ELARGEHIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E4HIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 1024)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 1024)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 1024), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(1024, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "e4gan = E4HIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E6HIVGAN(dc.models.WGAN):\n",
    "\n",
    "  def get_noise_input_shape(self):\n",
    "    return (None, 1024)\n",
    "\n",
    "  def get_data_input_shapes(self):\n",
    "    return [(None, 1024)]\n",
    "\n",
    "  def create_generator(self, noise_input, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=noise_input, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.sigmoid, normalizer_fn=tf.layers.batch_normalization)\n",
    "    reshaped = layers.Reshape((None, 1024), in_layers=dense5)\n",
    "    return [reshaped]\n",
    "\n",
    "  def create_discriminator(self, data_inputs, conditional_inputs):\n",
    "    dense1 = layers.Dense(1024, in_layers=data_inputs, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense2 = layers.Dense(1024, in_layers=dense1, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense3 = layers.Dense(1024, in_layers=dense2, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense4 = layers.Dense(1024, in_layers=dense3, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense5 = layers.Dense(1024, in_layers=dense4, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense6 = layers.Dense(1024, in_layers=dense5, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    dense7 = layers.Dense(1, in_layers=dense6, activation_fn=tf.nn.relu, normalizer_fn=tf.layers.batch_normalization)\n",
    "    return dense7\n",
    "#DISCRIMINATOR NEEDS TO BE BETTER!\n",
    "e6gan = E6HIVGAN(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-7ffc8a9b71d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mesmallgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mesmallgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mesmalliterbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/models/gan.py\u001b[0m in \u001b[0;36mfit_gan\u001b[0;34m(self, batches, generator_steps, max_checkpoints_to_keep, checkpoint_interval, restore)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0msubmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_submodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             checkpoint_interval=0)\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mdiscrim_average_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, feed_dict_generator, max_checkpoints_to_keep, checkpoint_interval, restore, submodel)\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary_op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m           \u001b[0mfetched_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def esmalliterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(esmallgan.batch_size)\n",
    "    for batch in eshiv_train_dataset.iterbatches(batch_size=esmallgan.batch_size):\n",
    "      yield {esmallgan.data_inputs[0]: batch[0]}\n",
    "\n",
    "esmallgan.fit_gan(esmalliterbatches(10), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-accada4f8b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0memediumgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0memediumgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memediumiterbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/models/gan.py\u001b[0m in \u001b[0;36mfit_gan\u001b[0;34m(self, batches, generator_steps, max_checkpoints_to_keep, checkpoint_interval, restore)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0msubmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_submodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 checkpoint_interval=0)\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mgen_average_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, feed_dict_generator, max_checkpoints_to_keep, checkpoint_interval, restore, submodel)\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary_op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m           \u001b[0mfetched_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def emediumiterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(emediumgan.batch_size)\n",
    "    for batch in emhiv_train_dataset.iterbatches(batch_size=emediumgan.batch_size):\n",
    "      yield {emediumgan.data_inputs[0]: batch[0]}\n",
    "\n",
    "emediumgan.fit_gan(emediumiterbatches(10), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a0b39b501fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0megan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0megan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meiterbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/models/gan.py\u001b[0m in \u001b[0;36mfit_gan\u001b[0;34m(self, batches, generator_steps, max_checkpoints_to_keep, checkpoint_interval, restore)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0msubmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_submodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 checkpoint_interval=0)\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mgen_average_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/deepchem/models/tensorgraph/tensor_graph.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, feed_dict_generator, max_checkpoints_to_keep, checkpoint_interval, restore, submodel)\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary_op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m           \u001b[0mfetched_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def elargeiterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(elargegan.batch_size)\n",
    "    for batch in elhiv_train_dataset.iterbatches(batch_size=elargegan.batch_size):\n",
    "      yield {elargegan.data_inputs[0]: batch[0]}\n",
    "\n",
    "elargegan.fit_gan(elargeiterbatches(10), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e4iterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(e4gan.batch_size)\n",
    "    for batch in e4hiv_train_dataset.iterbatches(batch_size=e4gan.batch_size):\n",
    "      yield {e4gan.data_inputs[0]: batch[0]}\n",
    "\n",
    "e4gan.fit_gan(e4iterbatches(10), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e6iterbatches(epochs):\n",
    "  for i in range(epochs):\n",
    "    print(e6gan.batch_size)\n",
    "    for batch in e6hiv_train_dataset.iterbatches(batch_size=e6gan.batch_size):\n",
    "      yield {e6gan.data_inputs[0]: batch[0]}\n",
    "\n",
    "e6gan.fit_gan(e6iterbatches(10), generator_steps=1.5, checkpoint_interval=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
